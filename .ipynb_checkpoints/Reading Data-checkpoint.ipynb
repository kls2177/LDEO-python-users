{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading and Writing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A) mat files (scipy.io)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "from scipy.io import loadmat as loadmat #this is the scipy module that loads .mat files\n",
    "from scipy.io import savemat as savemat #this is the scipy module that saves .mat files\n",
    "\n",
    "uri = 'http://www.karensmith.squarespace.com/storage/python_test.mat' #can't seem to get loadmat to work with a uri\n",
    "\n",
    "matfile = loadmat('python_test.mat')  # load .mat file\n",
    "\n",
    "array1 = matfile['array1']\n",
    "array2 = matfile['array2']\n",
    "\n",
    "array1.shape\n",
    "\n",
    "type(array1)\n",
    "\n",
    "savemat('python_test_save.mat',{'array1':array1}) #save numpy array to .mat file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B) csv, txt, xls with pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd #great for reading .csv and .txt files\n",
    "\n",
    "uri1 = 'http://www.ldeo.columbia.edu/~rpa/usgs_earthquakes_2014.csv' #example from Ryan's worskshop\n",
    "\n",
    "d1 = pd.read_csv(uri1,index_col='time') #many argument options (see pandas website for all the details)\n",
    "\n",
    "type(d1)\n",
    "\n",
    "d1.head()\n",
    "\n",
    "uri2 = 'http://karensmith.squarespace.com/storage/python_test.csv'\n",
    "\n",
    "d2 = pd.read_csv(uri2) #default case (no arguments)\n",
    "\n",
    "d2.head()\n",
    "\n",
    "d2 = pd.read_csv(uri2,index_col=1) #can pass arguments to specify column order\n",
    "\n",
    "print(d2)\n",
    "\n",
    "uri3 = 'http://karensmith.squarespace.com/storage/python_test.xls'\n",
    "\n",
    "d3 = pd.read_excel(uri3) #pandas can also be used to read .xls files\n",
    "\n",
    "d3.head()\n",
    "\n",
    "d1.to_csv('earthquakes_test.csv') #writing our d1 DataFrame object to a .csv file\n",
    "\n",
    "d2.to_excel('new_python_test.xls', sheet_name='Sheet1') #writing our d2 DataFrame object to a .xls file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C) netcdf4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from netCDF4 import Dataset\n",
    "\n",
    "uri = 'http://iridl.ldeo.columbia.edu/SOURCES/.NOAA/.NCDC/.ERSST/.version4/anom/.sst/T/(days%20since%201960-01-01)/streamgridunitconvert/dods'\n",
    "\n",
    "#use 'Dataset' to read file as netcdf4\n",
    "nc = Dataset(uri)\n",
    "\n",
    "nc\n",
    "\n",
    "SST = nc.variables['sst'][:,0] #this is the same as ['sst'[:,0,:,:]] -> gets rid of a degenerate dimension (same as squeeze in matlab)\n",
    "Lat = nc.variables['Y'][:]\n",
    "Lon = nc.variables['X'][:]\n",
    "\n",
    "#write a new netcdf file\n",
    "new_nc = Dataset('python_test.nc', 'w', format='NETCDF3_CLASSIC')\n",
    "new_nc.description = 'Example data'\n",
    "\n",
    "# define dimensions\n",
    "new_nc.createDimension('time', None) #record dimension\n",
    "new_nc.createDimension('lat', 72)\n",
    "new_nc.createDimension('lon', 144)\n",
    "\n",
    "# define variables\n",
    "times = new_nc.createVariable('time', 'f8', ('time',))\n",
    "latitudes = new_nc.createVariable('latitude', 'f4', ('lat',))\n",
    "longitudes = new_nc.createVariable('longitude', 'f4', ('lon',))\n",
    "tmp = new_nc.createVariable('tmp', 'f4', ('time', 'lat', 'lon',))\n",
    "\n",
    "# allocate data\n",
    "lats =  np.arange(-90, 90, 2.5)\n",
    "lons =  np.arange(-180, 180, 2.5)\n",
    "latitudes[:] = lats\n",
    "longitudes[:] = lons\n",
    "for i in range(5):\n",
    "    tmp[i,:,:] = np.random.uniform(size=(len(lats), len(lons))) #default uniform distribution between 0 and 1\n",
    "\n",
    "new_nc.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D) netcdf3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.io import netcdf #scipy.io can only read/write netcdf3\n",
    "\n",
    "#read the file we just created above using netcdf4\n",
    "f = netcdf.netcdf_file('python_test.nc', 'r')\n",
    "f\n",
    "\n",
    "\n",
    "print(f.description)\n",
    "lat = f.variables['latitude']\n",
    "print(lat.shape)\n",
    "print(lat[:])\n",
    "\n",
    "f.close()\n",
    "#data has to be copied to main memory if we want to process data after we close the netcdf file (see message below)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E) hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Using numpy to import regular columns of data from .CSV file\n",
    "signal = numpy.loadtxt(file_location_and_name, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
